---
layout: post
title: "[DL] 1. Deepf Feedforward Networks"
category: study
tags: ai
---

> [Deep Learning]을 정리하겠다.



## Questions
**Q1.** How do maxout units helps in dealing with the vanishing gradient problem? <br>
**A1.** ReLu가 전형적인 maxout의 예이다. 각 입력 그룹에 해당하는 선형함수들의 최댓값들만을 취함으로써, 기울기 소멸(vanishing, saturation)에서 자유로워지는데 도움이 된다.

**Q2.** Are there specific criteria or techniques that can guide the process of choosing the most suitable activation functions in neural network? <br> 
**A2.** 일반적으로, Recurrent 모델은 hyper-tanzent를 쓰고, 그 밖에는 ReLU를 가장 많이 쓴다.

**Q3.** How can we determine the width of hidden layers and the amount of nodes in each hidden layers? <br>
**A3.** Basiyan Optimization 기법 등을 사용해 찾아갈 순 있지만, 정답은 없다.

**Q4.** 각 레이어에는 동일한 activation function이 존재한다. 만약 동일한 레이어에 노드마다 다른 activation function을 적용한다면 비선형적인 상황에도 대처할 수 있지 않을까?
**A4.** 좋은 발상이지만, 어떻게 조합하게 할 것인지 아이디어가 존재하지 않는다. 어떤 조합이 어떤 상황에 좋은지 먼저 검토해야겠다. 더불어 모델을 만둘기 어려워진다는 단점도 있다. 대학원 가서 해보는 것은 어떤가?


<!-- Links -->
[Deep Learning]: https://github.com/baejaeho18/MyLibrary/blob/main/Machine%20Learning/deeplearningbook.pdf
