---
layout: post
title: "[DL] 1. Deepf Feedforward Networks"
category: study
tags: ai
---

> [Deep Learning 강의]을 정리하겠다.


## Questions
**Q1.** How do maxout units helps in dealing with the vanishing gradient problem? <br>
**A1.** ReLu가 전형적인 maxout의 예이다. 각 입력 그룹에 해당하는 선형함수들의 최댓값들만을 취함으로써, 기울기 소멸(vanishing, saturation)에서 자유로워지는데 도움이 된다.

**Q2.** Are there specific criteria or techniques that can guide the process of choosing the most suitable activation functions in neural network? <br> 
**A2.** 일반적으로, Recurrent 모델은 hyper-tanzent를 쓰고, 그 밖에는 ReLU를 가장 많이 쓴다.

**Q3.** How can we determine the width of hidden layers and the amount of nodes in each hidden layers? <br>
**A3.** Basiyan Optimization 기법 등을 사용해 찾아갈 순 있지만, 정답은 없다.

**Q4.** 
**A4.**


<!-- Links -->
[Deep Learning 수업]: https://drive.google.com/drive/folders/1uhm0_U4edru6c-QWz0kMSSrPhh7pxZgg

