---
layout: post
title: "[DL] 1. Deep Feedforward Networks"
category: study
tags: ai
---

> [Deep Learning]을 정리하겠다.

**Deep feedforward networks**, 다른 말로는 **feedforward neural networks**(순방향 신경망) 혹은 **multilayer perceptrons(MLPs)**(다층 퍼셉트론)는 deep learning model의 본질과 다름 없다.
순방향신경망의 목적은 어떤 함수 f를 근사화하는 것이다.
예를 들어, $$y=f(x)$$ 인 분류기가 입력값 x를 y로 분류한다고 하자.
순방향 신경망(feedforward network)는 $$y=f(x;\seta)$$ 를 정의하고, 가장 잘 근사화한 파리미터 $$\seta$$ 를 학습한다.
입력값 x로부터 함수 f를 정의하고, 결과 y를 계산하기까지 **feedforward**로 정보가 흐른다.
모델의 결과 y로부터 다시 **feedback**을 받지는 않는다. 결과값 y가 입력값 x가 되지는 않는다는 뜻이다. 만약 그렇다면, 그 모델은 **recurrent neural networks**라고 불릴 것이다. 이는 자연어 처리에 유용하다.
한편, 객체 인지에 사용되는 **convolutional networks**는 사진에 특화된 순방향신경망이다.
그만큼 순방향 신경망은 다양한 학습모델의 기반이 된다.
<!--more-->
Feedforward neural networks가 **networks**라고 불리는 이유는 수많은 function들로 구성되어 있기 때문이다.
이 함수들이 어떻게 구성되어 있는지 보통 수식으로 표현한다.
$$f(x) = f^3(f^2(f^1(x)))$$
$$f^1$$ 은 **first layer** of the network이고, $$f^2$$ 는 **second layer**, 마지막 계층인 $$f^3$$ 은 **output layer**라고 부른다. 이렇게 중첩된 함수의 수를 **depth**라고 부른다. 입력층과 출력층 사이에 존재하는 계층들을 **hidden layer**라고 부른다.

한편, **neural**이라고 불리는 이유는 이러한 개념이 신경과학(neuroscience)에서 본따 만들어졌기 때문이다.
각 은닉층들은 일반적으로 vector-value로 표현된다. 계층벡터의 차원수가 곧 모델의 **width**인 셈이다. vector의 각 원소가 바로 neuron의 역할을 한다. 
계층을 vector-to-vector 함수로 생각할 수도 있지만, 뉴런과 같은 수많은 vector-to-scalar 함수 **unit**이 병렬로 동작하는 것으로 생각해도 좋다. 뉴런을 닮은 각 unit들은 다수의 unit으로부터 입력을 받고, 자체적인 activation value를 계산한다.
즉, 순방향 신경망은 뇌의 동작에서 영감을 받은, 통계적 일반화를 하기 위한 function approximation machine이다.

## FFNN and Linear Models
Logistic regression과 linear regression과 같은 선형모델은 효율적이고 안정적으로 학습한다. 해석이 명시적이고 빠른 closed form이나 convex optimization에서 학습히기 때문이다.
그러나 선형함수이기에, 둘 이상의 input의 상호작용을 이해할 수 없다. 언제나 독립적인 input이라고 가정한다는 뜻이다.

선형모델은 nonlinear function으로 확장하기 위해, 비선형 변환함수 φ(x)를 입력으로 사용한다. 이는  두 개의 입력 벡터를 비선형 함수로 변환하여 내적(inner product)을 계산하는 kernel trick과 동일하다.

그렇다면, 어떻게 φ를 결정할 수 있을까?
1.  


## Example: Learning XOR

## Gradient-Based Learning
### Cost Functions
### Output Units

## Hidden Units
## Architecture Design
## Back-Propagation



## Questions
**Q1.** How do maxout units helps in dealing with the vanishing gradient problem? <br>
**A1.** ReLu가 전형적인 maxout의 예이다. 각 입력 그룹에 해당하는 선형함수들의 최댓값들만을 취함으로써, 기울기 소멸(vanishing, saturation)에서 자유로워지는데 도움이 된다.

**Q2.** Are there specific criteria or techniques that can guide the process of choosing the most suitable activation functions in neural network? <br> 
**A2.** 일반적으로, Recurrent 모델은 hyper-tanzent를 쓰고, 그 밖에는 ReLU를 가장 많이 쓴다.

**Q3.** How can we determine the width of hidden layers and the amount of nodes in each hidden layers? <br>
**A3.** Basiyan Optimization 기법 등을 사용해 찾아갈 순 있지만, 정답은 없다.

**Q4.** 각 레이어에는 동일한 activation function이 존재한다. 만약 동일한 레이어에 노드마다 다른 activation function을 적용한다면 비선형적인 상황에도 대처할 수 있지 않을까?
**A4.** 좋은 발상이지만, 어떻게 조합하게 할 것인지 아이디어가 존재하지 않는다. 어떤 조합이 어떤 상황에 좋은지 먼저 검토해야겠다. 더불어 모델을 만둘기 어려워진다는 단점도 있다. 대학원 가서 해보는 것은 어떤가?


<!-- Links -->
[Deep Learning]: https://github.com/baejaeho18/MyLibrary/blob/main/Machine%20Learning/deeplearningbook.pdf
